# LoRA: Low-Rank Adaptation

LoRA - вид адаптеров, предложенный в статье [LoRA: Low-Rank Adaptation Of Large Language Models (Hu et al., 2021)](https://arxiv.org/abs/2106.09685).

В отличие от классических адаптеров, которые обучаются как модули-надстройки над слоями модели, адаптеры LoRA обучаются как низкоранговые аддитивные добавки к весам модели. Веса модели при этом остаются замороженными. Схематически это выглядит так:

![lora_scheme](lora_scheme.png)

Такая архитектурная особенность с одной стороны позволяет LoRA адаптерам не вносить дополнительную задержку в инференс, но с другой стороны затрудняет обработку батчей с входами для разных задач.
