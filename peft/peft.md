# Parameter efficient fine-tuning (PEFT)

## Введение

Раньше для того, чтобы заняться глубоким обучением, вы должны были иметь доступ к большому очищенному набору данных и самостоятельно разработать и обучить эффективную модель. Значит, проекты без существенной поддержки извне были невозможны по умолчанию. Однако за последние пару лет всё изменилось. Движущая сила такого роста — трансферное обучение.

**Трансферное обучение** подразумевает использование предварительно обученной модели, разработанной для конкретной задачи, и повторное использование ее в качестве отправной точки для новой, связанной задачи. Большая языковая модель, предварительно обученная на огромных объемах данных, уже усвоила широкий спектр языковых конструкций и знаний. 
![Fully fine tuned LLM generating response to user query](https://deci.ai/wp-content/uploads/2023/09/1-3-1024x576.png)
**Точная настройка** предполагает использование предварительно обученной модели и ее дальнейшее обучение новой задаче с новыми данными. Тонкой настройке обычно обучается вся предварительно обученная модель, включая все ее слои и параметры. Этот процесс может оказаться дорогостоящим и трудоемким, особенно для больших моделей.
![Parameter efficient fine-tuned LLM generating response to user query](https://deci.ai/wp-content/uploads/2023/09/4-6-1024x576.png)
**Точная настройка с эффективным использованием параметров** -  это метод точной настройки, который фокусируется на обучении только подмножества параметров предварительно обученной модели. Этот подход предполагает определение наиболее важных параметров для новой задачи и обновление этих параметров только во время обучения. Благодаря этому PEFT может значительно сократить объем вычислений, необходимых для точной настройки.

### Преимущества PEFT

1.  **Снижение затрат на вычисления и хранение:** PEFT включает в себя точную настройку лишь небольшого количества дополнительных параметров модели при одновременном замораживании большинства параметров предварительно обученных LLM, что значительно снижает затраты на вычисления и хранение.
2.  **Преодоление катастрофического забывания:** Во время полной настройки LLM может произойти катастрофическое забывание, когда модель забывает знания, полученные во время предварительного обучения. PEFT может решить эту проблему, обновив лишь несколько параметров.
3.  **Более высокая производительность в режимах с малым объемом данных:** Было показано, что подходы PEFT работают лучше, чем полная точная настройка в режимах с небольшим объемом данных, и лучше обобщаются на сценарии, выходящие за пределы предметной области.
4.  **Переносимость:** методы PEFT позволяют пользователям получать крошечные контрольные точки размером в несколько МБ по сравнению с большими контрольными точками

## Методы PEFT

### Адаптеры (Adapters)

Адаптеры - это один из PEFT-методов, который был предложен в статье [Parameter-Efficient Transfer Learning for NLP (Houlsby et al., 2019)](https://arxiv.org/abs/1902.00751). Рассмотрим следующую постановку проблемы:

У нас есть набор задач (например, в области NLP), которые мы хотели бы научиться решать с помощью модели машинного обучения. У нас нет возможности собрать достаточное количество данных и обучить по отдельной модели с нуля на каждую из этих задач. Зато данных должно хватить для дообучения какой-нибудь очень большой предобученной модели. На этом этапе мы сталкиваемся с рядом трудностей:
* модель *большая*, полное дообучение с разморозкой всех параметров может оказаться слишком дорогим, а разморозка малого числа параметров может не дать желаемого качества;
* даже если у нас есть возможность дообучать модель, теперь нам необходимо хранить по одной версии этой *большой* модели на каждую из задач, что может быть неудобно и дорого;

Адаптеры предлагают подход к решению этих проблем. Вместо дообучения весов модели, предлагается обучать небольшие модули, которые размещались бы после каких-нибудь слоев и изменяли бы их выходы. 

В статье адаптеры применяют для дообучения BERT'а на 26 разных задач. В ней адаптеры имеют форму ботлнек-слоя с функцией активации посередине и skip connection'ом. Располагаются они после каждого полносвязного слоя в сети. Суммарный размер всех адаптеров для одной задачи составил порядка ~1-3% от размера всей модели.

![bert_adapters](images/bert_adapters.png)

По итогу получилось, что качество модели с адаптерами сопоставимо с качеством полностью дообученной модели. Немного увеличилось время инференса, но зато выше перечисленных проблем теперь можно избежать:
* адаптеры *маленькие*, обучать их не так дорого как всю модель;
* для хранения разных версий модели теперь не надо много места.

В качестве приятного бонуса: можно организовать инференс так, чтобы одновременно обрабатывать батч, состоящий из запросов по разным задачам - одновременно работать с несколькими версиями адаптеров не так страшно, как с несколькими версиями дообученной основной модели.

Позднее появилось много разных видов адаптеров. Идеи некоторых из них мы еще разберем в рамках этой главы. Стоит отметить, что есть такой ресурс как [AdapterHub](https://adapterhub.ml/). Это репозиторий по типу Hugging Face Hub'а для трансформеров, в котором хранятся адаптеры всех сортов для всевозможных моделей и задач (а если чего-то нет, можно контрибьютить). Для него есть документация и руководства, он совместим с библиотекой [transformers](https://github.com/huggingface/transformers). Ссылки:

* AdapterHub - https://adapterhub.ml/
* Руководство по AdapterHub от Hugging Face - https://huggingface.co/docs/hub/adapter-transformers
* GitHub аддона с адаптерами к библиотеке transformers - https://github.com/adapter-hub/adapters


### Prompt tuning
Первоначальная концепция prompt tuning относится к методам, которые изменяют входной промпт для достижения лучших результатов. Например, предположим, что мы хотим перевести английское предложение на немецкий. Мы можем задать модели различные вопросы, как показано ниже:

![Alt text](images/prpt.png)

Подход, проиллюстрированный выше, называется hard prompt tuning, поскольку мы напрямую меняем дискретные входные токены, которые не поддаются обновлению с помошью обратного распространения.

В отличие от hard prompt tuning, soft prompt tuning [Lester et al., 2021](https://arxiv.org/abs/2104.08691) объединяет ембеддинги входных токенов с обучаемым тензором, который может быть оптимизирован с помощью обратного распространения.

Soft prompts отличаются от hard prompts тем, что они обучаются путем обратного распространения и, таким образом, корректируются на основе лосса на наборе данных.

Soft prompt tuning значительно более эффективен с точки зрения параметров, чем full file tuning, хотя качество может быть немного хуже.


### Prefix tuning

Prefix tuning - это более легковесная альтернатива fine tuning больших языковых моделей [Liang 2021](https://arxiv.org/abs/2101.00190). Точная настройка требует обновления и сохранения всех параметров модели для каждой задачи, что может быть очень затратным, учитывая большой размер существующих моделей. Настройка префикса сохраняет параметры языковой модели замороженными и оптимизирует небольшой непрерывный вектор, специфичный для конкретной задачи, называемый префиксом. *Префикс* - это набор свободных параметров, которые обучаются для решения конкретной задачи.

Префикс можно рассматривать как последовательность “виртуальных токенов”, к которым могут обращаться последующие токены. Обучая только 0,1% параметров, prefix tuning обеспечивает производительность, сравнимую с full fine tuning при настройке полных данных, превосходит full fine tuning при дообучении на малом объеме данных.

![Alt text](images/prft_1.png)

*Prefix - последовательность виртуальных токенов*

Обозначим за $h_i \in \mathbb{R}^d$ вектор активаций в момент времени $i$, где $h_i = [h_i^{(1)},..,h_i^{(n)}]$ представляет собой объединение всех слоёв активации на данном временном шаге, и $h_i^{(j)}$ это вектор активации $j$ слоя в момент времени $i$.

Prefix-tuning добавляет префикс к autoregressive LM для получения $z = [Prefix;x;y]$, или добавляет префиксы для encoder и decoder: $z = [Prefix;x;Prefix';y]$.

Обозначим за $P_{\theta}$ обучаемую матрицу свободных параметров размерности $|P_{idx}| \times dim(h_i)$

![Alt text](images/prft_2.png)
 
  *При вычислении векторов активаций обрабатываем токены z по одному, если i - тый токен принадлежит префиксу, то вектор активации это i-тая строка $P_{\theta}$, иначе это вектор активаций модели после обработки $z_i$*

На практике выяснилось, что прямое обновление $P_{\theta}$ ведёт нестабильной оптимизации и падению производительности. Поэтому будем обучать матрицу меньшей размерности и с помощью полносвязной сети приводить вектора к нужному размеру. Теперь обучается P' и параметры MLP(multi layer perceptron)

$P_{theta}[i:] = MLP_{\theta}(P'_{theta}[i:])$

*NB: кол-во строк в P и P' должно совпадать*

#### Рассмотрим применение prefix tuning к трансформеру.

Мы добавляем обучаемый тензор к каждому блоку трансформера (в отличие от soft prompt tuning, где обучаемый тензор добавляется только ко входным эмбеддингам). Также мы дополнительно преобразуем данный тензор с помошью полносвязной сети (двухслойный перцептрон с активацией по середине). На рисунке ниже показана разница между обычным блоком трансформера и модифицированным с помощью префикса:

![Alt text](images/prft_3.png)

При использовании prefix tuning обновляются только префиксы, в то время как остальные слои фиксируются и не обновляются.

### P-Tuning
Промтинг - написанные вручную шаблоны подсказок, которые используют в качестве дополнительных входных данных для языковых моделей. Он позволяет улучшить результаты предобученных языковых моделей.
Однако у промтинга есть проблемы со стабильностью, поскольку качество будет сильно зависить от каждого дополнительного слова. То есть даже изменив одно слово, качество результатов может значительно упасть. 

<img src="images/PT_ineffective_promts.png" width="300">
                                                    
Было несколько работ с попытками найти способ подбора наиболее эффективного промта[[1](https://arxiv.org/abs/2103.10385), [2](https://arxiv.org/abs/2010.15980), [3](https://arxiv.org/abs/2012.15723)].

Для решения этой проблемы с другой стороны в [статье](https://arxiv.org/abs/2103.10385) предложили P-Tuning. Идея заключаетсяя в том, чтобы не только делать промт более детальным или эффективным, а ещё обучить непрерывное представления промта для большей стабильности.

<img src="images/PT.png" width="900">

Если раньше дописывали промт $P$ к нашему тексту $X$, пропускали $concat[P, X]$ через слой эмбеддингов, получая $concat[e(P), e(X)]$, то теперь мы сначала некторорым энкодером $H$ получаем эмбеддинги $h(P)$ для $P$ и затем конкатенируем с $e(X)$, получая $concat[h(P), e(X)]$. 

<img src="images/PT_metrics.png" width="600">

Так в задаче Knowledge probing P-Tuning значительно превосходит стандартный промтинг.

### Vision-Language Prompt Tuning
Prompt tuning можно использовать и для задач, связанных не только с языком, но и с мультимодальными данными, например в задачах Visual-Question-Answering (VQA). Когда на вход подается картинка и какой-то вопрос о ней. Нам о VQA важно знать следующее: от картинки и от текста независимо берутся эмбеддинги, а затем уже над ними производятся какие-то операции. А значит можно использовать Prompt Tuning для этих эмбеддингов.

И если хочется построить систему, которая будет отвечать, например на 10 разных вопросов о входной картинке, то можно попробовать использовать Prompt Tuning следующими способами.

![Screen Shot 2023-12-11 at 23.19.29.png](images/vqa_fl_Screen_Shot_2023-12-11_at_23.19.29.png)

- `CoOp` - Context Optimization (Text Prompt Tuning). Prompt Tuning применяется только к текстовым эмбеддингам;
- `VPT` - Visual Prompt Tuning. Prompt Tuning применяется только к эмбеддингам картинок;
- `UPT` - Unified Prompt Tuning. Prompt Tuning применяется и к текстовым эмбеддингам и к эмбеддингам картинок.

Выбор метода зависит от природы задач, которые мы хотим решать. Если для задач у нас один пул картинок, но по каждой картинке мы хотим научиться отвечать на несколько разных вопросов, то следует использовать `CoOp`. Однако, если модель вопроса у нас одна, а картинок много, то лучше воспользоваться `VPT`, и `UPT` наверное является наиболее универсальным подходом, который стоит использовать по умолчанию.

Можно подумать, а какой еще информацией мы обладаем о природе нашей задачи? Например, некоторые вопросы могут быть похожи друг на друга, и тогда имеет смысл, чтобы у них в конечном итоге эмбеддинги были более похожим. Оказывается, что можно модифицировать процесс `UPT` таким образом, чтобы учесть это и он описан в статье "Multitask Vision-Language Prompt Tuning" ([arxiv](https://arxiv.org/abs/2211.11720)).

![Screen Shot 2023-12-11 at 23.21.36.png](images/vqa_fl_Screen_Shot_2023-12-11_at_23.21.36.png)

Мы можем сгруппировать похожие задачи и обучать эмбеддинги для схожих задач совместно. Основная задача, которая здесь возникает - сгруппировать похожие задачи между собой, и для групп похожих задач задавать одни и те же префиксы в Prompt Tuning. Определить схожесть задач можно по тому, на сколько хорошо модель обученная на задаче A применима к задаче B. На рисунке как раз показаны такие замеры

![Screen Shot 2023-12-12 at 12.14.20.png](images/vqa_fl_Screen_Shot_2023-12-12_at_12.14.20.png)

При таком подходе к файнтюнингу модели в результате получаем модель, которая в целом ведет себя лучше, чем при обучении на каждую из задач по-отдельности.

![Screen Shot 2023-12-12 at 12.17.09.png](images/vqa_fl_Screen_Shot_2023-12-12_at_12.17.09.png)

---

Методы, которые мы уже обсудили применимы в случаях, когда при файнтюнинге у нас имеется доступ ко всем данным сразу. Однако, не все модели обучаются на одном компьютере / кластере с доступом ко всем обучающим данным. Порой случается так, что данные распределены на разных компьютерах и сбор их в одно место не представляется возможным, но обучение моделей все равно хочется производить. Такой подход к обучению называется Federated Learning. 

![Screen Shot 2023-12-12 at 12.23.59.png](images/vqa_fl_Screen_Shot_2023-12-12_at_12.23.59.png)

### LoRA: Low-Rank Adaptation

LoRA - вид адаптеров, предложенный в статье [LoRA: Low-Rank Adaptation Of Large Language Models (Hu et al., 2021)](https://arxiv.org/abs/2106.09685).

В отличие от классических адаптеров, которые обучаются как модули-надстройки над слоями модели, адаптеры LoRA обучаются как низкоранговые аддитивные добавки к весам модели. Веса модели при этом остаются замороженными. Схематически это выглядит так:

![lora_scheme](images/lora_scheme.png)

$A$ и $B$ - матрицы скелетного разложения матрицы-добавки $BA$, $rank(BA) = r$. Перед началом обучения матрица $A$ инициализируется нормальным шумом, а матрица $B$ - нулями. Такая инициализация на старте обучения зануляет добавку, что соответствует обычному инференсу замороженной модели.

Во время инференса, матрицы LoRA адаптеров складываются с весами модели. Это, по сравнению с обычными адаптерами, позволяет не создавать дополнительной задержки, но, с другой стороны, затрудняет обработку батчей с входами из разных задач.

### $(IA)^3$
Нужны PEFT методы, обладающие следующими свойствами:
 - Добавлять и обнавлять как можно меньше параметров;
 - Достигать высокой точности после few-shot обучения на новой задаче;
 - Допускать возможность решать несколько задач.

Чтобы легко обучаться батчами со смешанными задачами, метод PEFT в идеале не должен изменять саму модель. В противном случае каждый пример в батче фактически должен был бы обрабатываться другой моделью. Более удобной альтернативой являются методы, которые непосредственно изменяют активации модели, поскольку это может быть сделано независимо и дешево для каждого примера в батче в соответствии с тем, какой задаче соответствует пример. 

В [статье](https://arxiv.org/pdf/2205.05638.pdf) предлагается новый PEFT метод: $(IA)^3$, обладающий такими свойствами.  Идея в том, чтобы обучать масштабирующие векторы для некоторых наборов активаций для модели Трансформер.

<img src="images/ia3.png" width="600">

Стоит отметить, что если теперь нужно использовать модель только для одной задачи, то достаточно перемножить полученные ваетора на матрицы и в дальшейшем не потребуется поэлементного умножения, тем самым вычислительная стоимость модели до и после будут одинаковыми.

Авторы сравнили $(IA)^3$ с другими PEFT методами:

<img src="images/ia3_metrics.png" width="300">

Тем самым, обновляя только 0.01% параметров исходной модели, $(IA)^3$ превосходит другие методы по точности.

### PEFT в Federated Learning

При Federated Learning у нас есть главная копия модели, которая рассылается на устройства, где она дообучается на локальных данных, после чего обновленная версия модели отправляется для синхронизации на главный компьютер. Там главная копия модели обновляется и новая версия снова отправляется на устройства.

Cамым узким местом в этом подходе является пересылка данных, и их синхронизация в главном компьютере. Она занимает в разы больше времени, чем непосредственно обучение модели. И естественным образом мы хотим воспользоваться каким-то эффективным методом файнтюнинга. Что приходит в голову:

- Использовать LoRA для того, чтобы не пересылать всю модель, а отправлять лишь Low Rank матрицы;
- И в принципе мы готовы увеличить время одной итерации непосредственно самого обучения модели, если от этого количество итераций, включающих пересылку станет меньше.

Такими мыслями руководствовались авторы статьи "FedPara: Low-Rank Hadamard Product For Communication-Efficient Federated Learning" ([arxiv](https://arxiv.org/abs/2108.06098)).

Вспомним еще раз, что LoRA - **Low** Rank Adaptation, т.е. матрица, которой мы аппроксимируем исходную является низкоранговой, что не очень хорошо, т.к. она не несет в себе много информации. Следовательно хотелось бы повысить ранг этой матрицы. Это можно сделать, если приближать исходную матрицу не произведением двух, а следующим образом:

![Screen Shot 2023-12-12 at 12.30.50.png](images/vqa_fl_Screen_Shot_2023-12-12_at_12.30.50.png)

Такой подход, при том же количестве параметров, что и у LoRA позволяет аппроксимировать матрицу ранга $R^2$ вместо исходного $R$, на что в том числе указывает и таблица:

![Screen Shot 2023-12-12 at 12.32.24.png](images/vqa_fl_Screen_Shot_2023-12-12_at_12.32.24.png)

Однако это повышение ранга матрицы дается не бесплатно, и такая операция делает "дороже" одну итерацию непосредственно обучения. Например для модели VGG16 в исходном варианте время на forward + backward pass одного батча уходит время $1.64 sec$ в то время как при параметризации FedPara это займет $2.34 sec$. Однако видно, на сколько быстрее  модель с FedPara сходится к тому же результату, что и оригинальная модель. И на сколько меньше данных для этого потребовалось переслать. Хотелось бы видеть сравнение еще и с версией, где использовалась LoRA, однако в статье таких сравнений нет.

![Screen Shot 2023-12-12 at 12.33.17.png](images/vqa_fl_Screen_Shot_2023-12-12_at_12.33.17.png)
